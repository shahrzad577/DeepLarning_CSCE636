{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps done for this task:\n",
    "\n",
    "1. Copy and paste short video for each category (push, other) manually in separate folder (each video contains either pushing or other activity--> this method made labeling automatically)\n",
    "2. Rename the videos based on their labeling. e.g. : push_01 , push_02, .., and  other_01, other_02, ..\n",
    "3. manually pasted all the renamed videos in one directory called video_dir\n",
    "4. Capture videos from video_dir and save the captured frames into image_dir\n",
    "5. configure the model \n",
    "*******************\n",
    "6. Since frames names start with either push or other, labels are assigned as 1 or zero automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This model is traied on Google Colab  with more number of data ( explained in report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2     # for capturing videos\n",
    "import matplotlib.pyplot as plt    # for plotting the images\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image   # for preprocessing the images\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np    \n",
    "import os\n",
    "import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I copied and pasted the raw videos manually in their corresponding folders.Then rename them corresponding to\n",
    "# their labels (for example: push_1 to push_100 and other_1 to other_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code need to be run only one time  \n",
    "\n",
    "def rename_file (path , name):\n",
    "    i=0                                         \n",
    "    for filename in os.listdir(path):\n",
    "        os.rename(os.path.join(path,filename), os.path.join(path, name + str(i)+'.avi'))\n",
    "        i = i +1\n",
    "\n",
    "all_video_push = \"D:\\\\tamu\\\\courses\\\\DeepLearning\\\\ProjectPart4\\\\pushing\"\n",
    "all_video_other = \"D:\\\\tamu\\\\courses\\\\DeepLearning\\\\ProjectPart4\\\\other\"\n",
    "        \n",
    "rename_file(all_video_push, 'push_')\n",
    "rename_file(all_video_other, 'other_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after renaming all videos (push and other) are manually pasted in one directory --> video_dir\n",
    "video_dir = \"D:\\\\tamu\\\\courses\\\\DeepLearning\\\\ProjectPart4\\\\all_video\"\n",
    "img_dir = \"D:\\\\tamu\\\\courses\\\\DeepLearning\\\\ProjectPart4\\\\all_img\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## capture video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path, dest_path):\n",
    "    file_name = video_path.split('.')[0].split('\\\\')[-1]\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    sec = 0\n",
    "    frameRate = 0.5           #//it will capture image in each 0.5 second\n",
    "    count = 1\n",
    "    hasFrames = True\n",
    "    while hasFrames:\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
    "        hasFrames,image = vidcap.read()\n",
    "        if hasFrames:\n",
    "            cv2.imwrite(dest_path+'\\\\'+file_name+'_'+ str(count)+\".jpg\", image)     # save frame as JPG file\n",
    "\n",
    "        count = count + 1\n",
    "        sec = sec + frameRate\n",
    "        sec = round(sec, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting captions from videos and put them in related derectories:\n",
    "\n",
    "for i in range(len(os.listdir(video_dir))):\n",
    "    video_path = video_dir + '\\\\' + os.listdir(video_dir)[i]\n",
    "    dest_path = img_dir\n",
    "\n",
    "    get_frames(video_path, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting labels\n",
    "assign 0 to the images labeled as 'other' and assign 1 to the images labeld as 'push'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label =[]\n",
    "for file in os.listdir(img_dir):\n",
    "    label = file.split('_')[0]\n",
    "    if label == 'other':\n",
    "        train_label.append(0)\n",
    "    else:\n",
    "        train_label.append(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "the number of video frmaes need to be the same. For this submission I manually handeled it to get equal number of frames for each video.\n",
    "Later I will use padding and masking layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing\n",
    "-resizing all images to (250,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xtrain(file_name):\n",
    "    X_train = []\n",
    "    for i , filename in enumerate(glob.glob(file_name)):\n",
    "        image = cv2.imread(filename)\n",
    "        res_img = cv2.resize(image, dsize=(250,250), interpolation=cv2.INTER_CUBIC)\n",
    "        X_train.append(res_img)\n",
    "    \n",
    "    return X_train \n",
    "\n",
    "X = get_Xtrain(img_dir + '\\\\*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    }
   ],
   "source": [
    "# checking the size of the resized images (X)\n",
    "X[0].shape\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get numpy array of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfering the train_list and label_list to array\n",
    "x_train = np.asarray(X)\n",
    "y_train = np.asarray(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (252, 250, 250, 3)\n",
      "y_train.shape:  (252,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape: ', x_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizing X dataset:\n",
    "x_train = x_train/x_train.max()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 10, 250, 250, 3)\n",
      "(24, 10)\n"
     ]
    }
   ],
   "source": [
    "# Chaning the shape of the input for TimeDistributed layer in the model\n",
    "x = []\n",
    "y = []\n",
    "for i in range(0, x_train.shape[0], 10):\n",
    "    x.append(x_train[i:(i+10)])\n",
    "    y.append(y_train[i:(i+10)])\n",
    "\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "x_train = x\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0], 10,1)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "Not using transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_6 (TimeDist (None, 10, 248, 248, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 10, 246, 246, 32)  4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 10, 246, 246, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 10, 123, 123, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 10, 123, 123, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 10, 123, 123, 32)  128       \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 10, 61, 61, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 10, 119072)        0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 5)             2381560   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10, 1)             6         \n",
      "=================================================================\n",
      "Total params: 2,386,782\n",
      "Trainable params: 2,386,718\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dropout,Flatten, MaxPooling2D, BatchNormalization, TimeDistributed\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "\n",
    "n_timesteps = 10                                         #  ==> nums of frames for each video (question above)\n",
    "lstm_cells =5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D (16, (3,3), activation = 'relu'),\n",
    "                                  input_shape = (n_timesteps, 250,250,3)))      \n",
    "model.add(TimeDistributed(Conv2D (32, (3,3), activation = 'relu')))             \n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(MaxPooling2D((2,2))))        \n",
    "#model.add(TimeDistributed(Conv2D(64, (3,3), activation = 'relu')))             \n",
    "#model.add(TimeDistributed(Conv2D(256, (3,3), activation = 'relu')))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
    "#model.add(TimeDistributed(Conv2D (512, (3,3), activation = 'relu')))\n",
    "#model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
    "#model.add(TimeDistributed(Conv2D (512, (3,3), activation = 'relu')))\n",
    "#model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
    "#model.add(TimeDistributed(BatchNormalization(momentum = momentum)))\n",
    "#model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Flatten()))                                                      #4\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "#model.add(LSTM(batch_size = n_batches, timesteps = n_timesteps, input_dim = (n_features,)))\n",
    "model.add(LSTM(units = lstm_cells, return_sequences = True, dropout=0.1, recurrent_dropout=0.5))\n",
    "#model.add(Dense(512, activation='relu'))     # you may need more Dense layers\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "#1: use conv2d to extract image features + wrap the subsequences through TimeDistributed\n",
    "#2: Dropout layer to overcome overfitting\n",
    "#3: MaxPooling the grab the most representative features from each featuremap\n",
    "#4: Flatten the tensors so as to be ready for LSTM layer\n",
    "#6: Final layer to predict the intended action (Binary output)\n",
    "## Note: Check if using pre-trained model outperforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(lr=2e-5)\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "# optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19 samples, validate on 5 samples\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = model.fit(x=x_train, y=y_train, validation_split = 0.2, batch_size=10, epochs=epochs, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
