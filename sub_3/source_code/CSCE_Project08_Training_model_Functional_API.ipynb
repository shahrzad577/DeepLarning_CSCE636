{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCE_Project08_Training_model_Functional_API.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQoIpZCvIHBd",
        "colab_type": "code",
        "outputId": "30c6627c-ea2d-4c73-b611-f125801ef66b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lwj1EaYf9yo",
        "colab_type": "code",
        "outputId": "d9303968-daf6-45a6-b59b-15e0f7b0339c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTBOb9r5TYsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Global variables:\n",
        "\n",
        "n_timesteps = 125    # n_timesteps should be the same as maxlen for padding\n",
        "vector_size = 75    # same as feature_size\n",
        "max_frame = n_timesteps\n",
        "cells = 2\n",
        "n_features = vector_size\n",
        "size = 250"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8cOiLG3Q5ca",
        "colab_type": "text"
      },
      "source": [
        "## Common functions utelized in both reading json files and images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxMhk7caQ9OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input = other_0_000000000000_rendered.png\n",
        "# output = other_0\n",
        "def get_file_name(file_name):\n",
        "    parts = file_name.split('_')\n",
        "    return parts[0] + '_' + parts[1]\n",
        "\n",
        "# input = 'push' or 'other'\n",
        "# output = 1 or 0\n",
        "def encode_label(label):\n",
        "    if 'push' in label:\n",
        "        return 1\n",
        "    elif 'other' in label:\n",
        "        return 0\n",
        "    else:\n",
        "        raise ValueError('label not supported')\n",
        "\n",
        "# input = array\n",
        "# output = normalized_array\n",
        "def normalized_value(array):\n",
        "    mean = array.mean()\n",
        "    std = array.std()\n",
        "    norm_arr = array - mean\n",
        "    norm_arr /= std\n",
        "    return norm_arr\n",
        "\n",
        "\n",
        "# input = video directory\n",
        "# output = list of video name: [other_0, other_1, ..., push_0, push_1, ..]\n",
        "def get_vidlist(video_dir):\n",
        "    vid_list = []\n",
        "    for file in os.listdir(video_dir):\n",
        "        vid_name = file.split('.')[0]\n",
        "        vid_list.append(vid_name)\n",
        "\n",
        "    return vid_list\n",
        "    \n",
        "\n",
        "# input = vid_list\n",
        "# output = train_list and val_list shuffled by name\n",
        "def get_train_val_list(vid_list, seed=6, split=0.8):\n",
        "    other = []\n",
        "    push = []\n",
        "    for vid_name in vid_list:\n",
        "        if 'other' in vid_name:\n",
        "            other.append(0)\n",
        "        elif 'push' in vid_name:\n",
        "            push.append(0)\n",
        "\n",
        "    split_other = int(len(other) * split)\n",
        "    split_push = int(len(push) * split)\n",
        "\n",
        "    train_vid_list = vid_list[0:split_other] + vid_list[len(other):len(other) + split_push]\n",
        "    val_vid_list = vid_list[split_other:len(other)] + vid_list[len(other) + split_push:]\n",
        "\n",
        "    random.Random(seed).shuffle(train_vid_list)\n",
        "    random.Random(seed).shuffle(val_vid_list)\n",
        "\n",
        "    return train_vid_list, val_vid_list\n",
        "\n",
        "\n",
        "# reshaping the target to align the shape of the x_train\n",
        "# input = list\n",
        "# output = expanded array on axis=0\n",
        "def add_first_dim(list):\n",
        "    arr = np.asarray(list)\n",
        "    return np.expand_dims(arr, axis=0)\n",
        "\n",
        "\n",
        "# reshaping the target to align the shape of the x_train\n",
        "# input = list\n",
        "# output = expanded array on axis=0\n",
        "def add_first_dim(list):\n",
        "    arr = np.asarray(list)\n",
        "    return np.expand_dims(arr, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbPPLqBRIy1Q",
        "colab_type": "text"
      },
      "source": [
        "## Reading Json files with Generator\n",
        "Although Json files are generally light enough and may not need generator. However, since the model is an API model and it is fit as model.fit_generator(), both input needs to be read in the same way.\n",
        "\n",
        "Notes about generator:\n",
        "1. The generator, generates data by batchsize (n-timesteps) and hence save memory\n",
        "2.  The generator needs to return a tuple of (x_train, target)\n",
        "3.  For some reason generator in Keras needs While loop (Stackoverflow)\n",
        "4. A generator by itself is an object and only could be called through a for loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB0TZfqsIRut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Note: in openpose, videos with no figure returns jsons with empty people object. These frames are waived when reading jsons.\n",
        "# So, The file names should be pass to reading images so the corresponding frames also get waived while reading images\n",
        "\n",
        "\n",
        "# input = json_file directory\n",
        "# output = a dic of file_name where json people are empty.\n",
        "def get_invalid_frames(json_dir):\n",
        "    no_fig_files = {}\n",
        "    for file in os.listdir(json_dir):\n",
        "        file_path = json_dir + '/' + file\n",
        "        temp = json.load(open(file_path))\n",
        "        if len(temp['people']) == 0:\n",
        "            parts = file.split('_')\n",
        "            img_name = parts[0] + '_' + parts[1] + '_' + parts[2] + '_rendered.png' #change to png\n",
        "            file_name = get_file_name(file)\n",
        "            no_fig_files.setdefault(file_name, []).append(img_name)\n",
        "\n",
        "    return no_fig_files\n",
        "\n",
        "# input = json_file directory , video_name (e.g: other_0)\n",
        "# output = list of list of features (vector of 75 size) relating to all frames corresponding to video_name \n",
        "def read_data_from_landmarks(json_dir,vid_name):\n",
        "    landmarks = []\n",
        "    for file in os.listdir(json_dir):\n",
        "        if vid_name == get_file_name(file):\n",
        "            file_path = json_dir + '/' + file\n",
        "            temp = json.load(open(file_path))\n",
        "            if len(temp['people']) == 0:\n",
        "                pass\n",
        "            else:\n",
        "                value = temp['people'][0]['pose_keypoints_2d']\n",
        "                norm_value = [float(i) / sum(value) for i in value]     # normalize the landmark features\n",
        "                landmarks.append(norm_value)\n",
        "\n",
        "    return landmarks\n",
        "\n",
        "\n",
        "#### Land_mark generator which yeild a tuple of padded_landmarks and corresponding label\n",
        "# This generator will later be called in model.fite_generator()\n",
        "def landmark_generator(vid_list, json_dir, pad_len=n_timesteps):\n",
        "    while True:\n",
        "        for vid_name in vid_list:\n",
        "            landmark = read_data_from_landmarks(json_dir, vid_name)\n",
        "            padded = pad_sequences([landmark], dtype='float32', maxlen=pad_len, padding='post')\n",
        "            batch = (padded, add_first_dim(encode_label(vid_name)))\n",
        "            yield batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGA0W_xGOC_W",
        "colab_type": "text"
      },
      "source": [
        "## Reading captured frames with Generator\n",
        "The default function of keras for image generator (ImageDataGenerator) is not used since I could not handle times series in this function. Hence a generator is scripted.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1meh6O6K8UU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input = image_file\n",
        "# output = img_array\n",
        "def read_img(img_file):\n",
        "    img = cv2.imread(img_file)\n",
        "    img = cv2.resize(img, dsize=(size, size), interpolation=cv2.INTER_CUBIC)\n",
        "    return img\n",
        "\n",
        "\n",
        "# input= files in frame_dir and vid_name\n",
        "# output = arrays of all frames of one video, exluding invalid frames that are waived already in reading json files (frames with no figure)\n",
        "def get_array_of_frames(frame_dir, vid_name, json_dir):\n",
        "    images = []\n",
        "    invalid_frames = get_invalid_frames(json_dir)     # invalid frames of json needs to be waived in reading frames\n",
        "    for file in os.listdir(frame_dir):\n",
        "        if vid_name == get_file_name(file):\n",
        "\n",
        "            if vid_name in invalid_frames:\n",
        "                if file in invalid_frames[vid_name]:\n",
        "                    continue    #pass\n",
        "\n",
        "            array = read_img(frame_dir + \"/\" + file)\n",
        "            norm_array = normalized_value(array)\n",
        "            images.append(norm_array)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "#### image generator which yeild a tuple of padded_arrays of images and corresponding label\n",
        "# This generator will later be called in model.fite_generator()\n",
        "def image_generator(vid_list, frame_dir, json_dir, pad_len=n_timesteps):\n",
        "    while True:\n",
        "        for vid_name in vid_list:\n",
        "            frames = get_array_of_frames(frame_dir, vid_name, json_dir)\n",
        "            padded = pad_sequences([frames], dtype='float32', maxlen=pad_len, padding='post')\n",
        "            batch = (padded, add_first_dim(encode_label(vid_name)))\n",
        "            yield batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jOnEmhsKTNa",
        "colab_type": "text"
      },
      "source": [
        "## Model Configuration\n",
        "Using Functional API to creat 2 branches for 2 types of input (json and image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMUGQzyorhAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## import libraries\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "#from keras.layers.convolutional import Conv3D\n",
        "#from keras.layers.pooling import MaxPooling3D\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.applications import MobileNet\n",
        "from keras.layers import TimeDistributed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1lES-LCISBc",
        "colab_type": "code",
        "outputId": "911af11e-36bb-4032-9d3a-80d3d123d0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "# Json\n",
        "landmark_input = Input(shape=(max_frame, vector_size),\n",
        "               name='landmark')           \n",
        "lstm1 = LSTM(units=cells, dropout=0.1, recurrent_dropout=0.5, return_sequences=True)(landmark_input)\n",
        "flat1 = Flatten()(lstm1)\n",
        "\n",
        "# image\n",
        "image_input = Input(shape=(max_frame, size, size, 3), name = 'image')\n",
        "base_model = MobileNet(input_shape=(size, size, 3),\n",
        "                       include_top=False,\n",
        "                       weights='imagenet',\n",
        "                       input_tensor=None,\n",
        "                       pooling='avg',\n",
        "                       classes=2)\n",
        "\n",
        "im = TimeDistributed(base_model)(image_input)\n",
        "im = LSTM(units=cells, return_sequences=True)(im)\n",
        "flat2 = Flatten()(im)\n",
        "\n",
        "merge = concatenate([flat1, flat2])\n",
        "#dr = Dropout(0.5)(merge)\n",
        "\n",
        "hidden = Dense(1, activation='relu')(merge)\n",
        "output = Dense(1, activation='sigmoid')(hidden)    # softmax\n",
        "\n",
        "model = Model([landmark_input,image_input], output)\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  warnings.warn('`input_shape` is undefined or non-square, '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              (None, 50, 250, 250, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "landmark (InputLayer)           (None, 50, 75)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 50, 1024)     3228864     image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 50, 2)        624         landmark[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 50, 2)        8216        time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 100)          0           lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 200)          0           flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            201         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            2           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,237,907\n",
            "Trainable params: 3,216,019\n",
            "Non-trainable params: 21,888\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB7ewFFMsLaU",
        "colab_type": "text"
      },
      "source": [
        "## Multiple generator\n",
        "Unlike model.fit() that can accept multiple inputs, model.fit_generator() can not accept multiple generators by default. Hence a function is added to make it work with multiple generators. Source_code: https://github.com/keras-team/keras/issues/8130"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptaZyZNrVpzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_multiple_generator(genX1, genX2):\n",
        "    while True:\n",
        "      try:\n",
        "        X1i = next(genX1)\n",
        "        X2i = next(genX2)\n",
        "        yield [X1i[0], X2i[0]], X1i[1]\n",
        "      except StopIteration:\n",
        "        print('This was the problemmmmmmmmmmmmmmmmmmm')\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14ArmI1Ct_07",
        "colab_type": "text"
      },
      "source": [
        "## Calling Generator objects and Get input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpKt6LA3izZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = \"/content/drive/My Drive/Google_Colab\"\n",
        "json_dir = base_dir + '/' + 'openpose_json'\n",
        "frame_dir = base_dir + '/' + 'video_frames'\n",
        "video_dir = base_dir + '/' + 'video'\n",
        "\n",
        "vid_list = get_vidlist (video_dir)\n",
        "train_list, val_list = get_train_val_list(vid_list, split=0.8)\n",
        "\n",
        "# calling json generator for train and validation\n",
        "train_json_generator = landmark_generator(train_list, json_dir)\n",
        "val_json_generator = landmark_generator(val_list, json_dir)\n",
        "\n",
        "# calling image generator for train and validation\n",
        "train_img_generator = image_generator(train_list, frame_dir,json_dir)\n",
        "val_img_generator = image_generator(val_list, frame_dir,json_dir)\n",
        "\n",
        "# calling generate_multiple_generator to get train_gen and val_gen\n",
        "train_gen = generate_multiple_generator(train_json_generator,train_img_generator)\n",
        "val_gen = generate_multiple_generator(val_json_generator, val_img_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LyjauLhv-Z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5hQfWm8bWEo",
        "colab_type": "code",
        "outputId": "3cab3d5a-7b11-46b5-8774-481a2a4dbd0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# fit the model with two inputs!\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "history=model.fit_generator(generator = train_gen,\n",
        "                        steps_per_epoch=len(train_list),\n",
        "                        epochs = epochs,\n",
        "                        validation_data = val_gen,\n",
        "                        validation_steps = len(val_list),\n",
        "                        use_multiprocessing=True,\n",
        "                        workers =1000)   #, workers = 2     and use_multiprocessing=True for Colab   \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo8hiZ3ceWx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "plot_model(model, to_file='model_8_API.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmjrtUnwojkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}